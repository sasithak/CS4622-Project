# -*- coding: utf-8 -*-
"""190332D_ML_Project_Label_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10bUhcRcQtAZDtzwV7Fdqz_RAuw5TfEVo
"""

# Loading data
import pandas as pd
train = pd.read_csv("/content/drive/My Drive/Semester7/ML/Project/Layer_7/train.csv")
valid = pd.read_csv("/content/drive/My Drive/Semester7/ML/Project/Layer_7/valid.csv")
test = pd.read_csv("/content/drive/My Drive/Semester7/ML/Project/Layer_7/test.csv")

train.head()

valid.head()

test.head()

"""##Distribution of labels

#Handling Missing Values

##Missing values in training data
"""

# Check for missing values in each label
labels = ["label_1", "label_2", "label_3", "label_4"]
train[labels].isnull().sum()

len(train["label_2"])

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
columns = ["label_2"]
imputer.fit(train[columns])

# Insert the missing values with the mean with the nearest integer
train[columns] = imputer.transform(train[columns]).round().astype("int")

# Recheck for missing values in each label to confirm whether there are no missing values left
train[labels].isnull().sum()

"""##Missing values in validation data"""

valid[labels].isnull().sum()

len(valid["label_2"])

imputer = SimpleImputer(strategy='mean')
columns = ["label_2"]
imputer.fit(valid[columns])

# Insert the missing values with the mean with the nearest integer
valid[columns] = imputer.transform(valid[columns]).round().astype("int")
valid[labels].isnull().sum()

"""#Label 1 - Speaker ID"""

# Split X and Y
x_label_1 = train.iloc[:, : -4]
y_label_1 = train["label_1"]
x_valid_label_1 = valid.iloc[:, : -4]
y_valid_label_1 = valid["label_1"]

x_label_1.head()

y_label_1.head()

"""##Class Distribution"""

import matplotlib.pyplot as plt

def plot_class_distribution(y):
  label_counts = y.value_counts()
  plt.figure(figsize=(12, 6))
  label_counts.plot(kind='bar')
  plt.xlabel('Value')
  plt.ylabel('Count')
  plt.title('Class Distribution')
  plt.xticks(rotation=45)
  plt.show()

plot_class_distribution(y_label_1)

"""`label 1` shows an even class distribition.

##Feature Selection
###Check for columns with low variance
"""

from sklearn.feature_selection import VarianceThreshold

def variance_check(x: pd.DataFrame, variance: float) -> []:
  try:
    var_thres = VarianceThreshold(variance)
    var_thres.fit(x)

    const_columns = [column for column in x.columns
                        if column not in x_label_1.columns[var_thres.get_support()]]

    print(f"The number of constant columns is {len(const_columns)}.")
    return const_columns

  except ValueError:
    print(f"No feature exists with variance grater than {variance}.")
    return []

const_columns_label_1 = variance_check(x_label_1, 0.3)

const_columns_label_1 = variance_check(x_label_1, 0.2)

const_columns_label_1 = variance_check(x_label_1, 0.18)

const_columns_label_1 = variance_check(x_label_1, 0.17)

"""Almost all columns have lower variance than 0.17. Therefore, I could not take any feature selection decision based on the variance.

###Check for correlation between features
"""

def correlation_check(x: pd.DataFrame, threshold: float) -> set:
  correlated_features = set()
  correlation_matrix = x.corr()
  for i in range(len(x.columns)):
      for j in range(i):
          if abs(correlation_matrix.iloc[i, j]) > threshold:
              colname = correlation_matrix.columns[i]
              correlated_features.add(colname)
  return correlated_features

corr = correlation_check(x_label_1, 0.9)
print(len(corr))

corr = correlation_check(x_label_1, 0.7)
print(len(corr))

"""There are 4 features with correlation value greater than 0.7. Dropping those columns."""

corr

x_label_1 = x_label_1.drop(corr, axis = 1)

x_valid_label_1 = x_valid_label_1.drop(corr, axis = 1)

"""###Mutual Information Classification"""

from sklearn.feature_selection import mutual_info_classif

def plot_mi(x: pd.DataFrame, y: pd.DataFrame) -> pd.Series:
  mi = pd.Series(mutual_info_classif(x, y))
  mi = mi.sort_values(ascending=False)
  mi.plot.bar(figsize=(32, 12))
  return mi

mi_label_1 = plot_mi(x_label_1, y_label_1)

"""Observing the distribution of the graph, selecting features with mutual information scre greater than 0.06."""

def feature_selection_mi(mi: pd.Series, x: pd.DataFrame, threshold: float) -> pd.DataFrame:
  selected_features = mi_label_1[mi_label_1 > threshold]
  print (f"Selected {selected_features.count()} features out of {len(mi_label_1)} features.")
  selected_cols = x.columns[selected_features.index]
  print(selected_cols)
  return selected_cols

selected_col_names_label_1 = feature_selection_mi(mi_label_1, x_label_1, 0.06)

x_label_1_selected = pd.DataFrame(x_label_1, columns = selected_col_names_label_1)
x_valid_label_1_selected = pd.DataFrame(x_valid_label_1, columns = selected_col_names_label_1)

"""##Modelling

###Model Evaluation
"""

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

def eval_model(y_actual: pd.DataFrame, y_predicted: pd.DataFrame, print_metrics = False) -> float:
  cm = confusion_matrix(y_actual, y_predicted)
  fp = cm.sum(axis=0) - np.diag(cm)
  fn = cm.sum(axis=1) - np.diag(cm)
  tp = np.diag(cm)
  tn = cm.sum() - (fp + fn + tp)

  accuracy = (tp + tn) / (tp + fp + tn + fn)
  err_rate = (fp + fn) / (tp + fp + tn + fn)
  sensitivity = tp / (tp + fn)
  specificity = tn / (tn + fp)
  precision = tp / (tp + fp)

  recall = sensitivity
  f_score = (2 * precision * recall) / (precision + recall)

  if (print_metrics):
    print(f"Accuracy:      {accuracy.mean():.4f}")
    print(f"Error Rate:    {err_rate.mean():.4f}")
    print(f"Sensitivity:   {sensitivity.mean():.4f}")
    print(f"Specificity:   {specificity.mean():.4f}")
    print(f"Precision:     {precision.mean():.4f}")
    print(f"F1 Score:      {f_score.mean():.4f}")

  return f_score.mean()

from sklearn.model_selection import cross_val_score

def scorer(estimator: any, X: pd.DataFrame, y: pd.DataFrame):
  y_pred = estimator.predict(X)
  f1_score = eval_model(y, y_pred)
  return f1_score

label_1_scores = []

"""###KNN Model"""

from sklearn.neighbors import KNeighborsClassifier

def knn_model(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, k: int):
  knn = KNeighborsClassifier(n_neighbors = k)
  f_score = cross_val_score(knn, x_train, y_train, cv = 5, scoring = scorer)
  knn.fit(x_train, y_train)
  y_pred = knn.predict(x_valid)
  eval_model(y_valid, y_pred, True)
  return [knn, f_score]

knn_label_1, f_score_knn_label_1 = knn_model(x_label_1_selected, y_label_1, x_valid_label_1_selected, y_valid_label_1, 13)
avg_f1_score_knn_label_1 = f_score_knn_label_1.mean()
print(f"Average F1 score of KNN cross validation: {avg_f1_score_knn_label_1}.")
label_1_scores.append([knn_label_1, "KNN", avg_f1_score_knn_label_1])

"""###Random Forest Model"""

from sklearn.ensemble import RandomForestClassifier

def rf_model(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, n_estimators: int, random_state: int):
  rf = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)
  f_score = cross_val_score(rf, x_train, y_train, cv = 5, scoring = scorer)
  rf.fit(x_train, y_train)
  y_pred = rf.predict(x_valid)
  eval_model(y_valid, y_pred, True)
  return [rf, f_score]

rf_label_1, f_score_rf_label_1 = rf_model(x_label_1_selected, y_label_1, x_valid_label_1_selected, y_valid_label_1, 100, 42)
avg_f1_score_rf_label_1 = f_score_rf_label_1.mean()
print(f"Average F1 score of RF cross validation: {avg_f1_score_rf_label_1}.")
label_1_scores.append([rf_label_1, "Random Forest", avg_f1_score_rf_label_1])

"""###XG Boost Model"""

import xgboost as xgb
import torch

def xgb_model_gpu(x_train, y_train, x_valid, y_valid, n_estimators, random_state):
  y_train_cpy = y_train.copy(deep=True)
  y_valid_cpy = y_valid.copy(deep=True)
  y_train_cpy = y_train_cpy - 1
  y_valid_cpy = y_valid_cpy - 1

  xgb_gpu = None
  if (torch.cuda.is_available()):
    xgb_gpu = xgb.XGBClassifier(
        n_estimators = n_estimators,
        random_state = random_state,
        tree_method = 'gpu_hist',
        gpu_id = 0,
    )
  else:
    xgb_gpu = xgb.XGBClassifier(n_estimators = n_estimators, random_state = random_state)

  f_score = cross_val_score(xgb_gpu, x_train, y_train_cpy, cv = 5, scoring = scorer)
  xgb_gpu.fit(x_train, y_train_cpy)
  y_pred = xgb_gpu.predict(x_valid)
  eval_model(y_valid_cpy, y_pred, True)

  return [xgb_gpu, f_score]

xgb_label_1, f_score_xgb_label_1 = xgb_model_gpu(x_label_1_selected, y_label_1, x_valid_label_1_selected, y_valid_label_1, 50, 42)
avg_f1_score_xgb_label_1 = f_score_xgb_label_1.mean()
print(f"Average F1 score of RF cross validation: {avg_f1_score_xgb_label_1}.")
label_1_scores.append([xgb_label_1, "XGBoost", avg_f1_score_xgb_label_1])

"""###Support Vector Machine Model"""

from sklearn.svm import SVC

def svm_model(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, kernel: str):
  svm = SVC(kernel = kernel)
  f_score = cross_val_score(svm, x_train, y_train, cv = 5, scoring = scorer)
  svm.fit(x_train, y_train)
  y_pred = svm.predict(x_valid)
  eval_model(y_valid, y_pred, True)
  return [svm, f_score]

kernels = ["linear", "rbf", "poly"]

for i in range(3):
  kernel = kernels[i]
  Kernel = kernel.capitalize()
  print(Kernel)
  svm, f_sc = svm_model(x_label_1_selected, y_label_1, x_valid_label_1_selected, y_valid_label_1, kernel)
  avg_f_sc = f_sc.mean()
  print(f"Average F1 score of SVM {Kernel} cross validation: {avg_f_sc}.")
  label_1_scores.append([svm, f"SVM {Kernel}", avg_f_sc])
  if (i!= 2): print()

"""###Final Model"""

for score in label_1_scores:
  print(score[1:])

def get_best_model(scores):
  max_index = 0
  for i in range(len(scores)):
    if scores[i][2] > scores[max_index][2]:
      max_index = i
  return scores[max_index][0]
