# -*- coding: utf-8 -*-
"""190332D_ML_Project_Label_7.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10bUhcRcQtAZDtzwV7Fdqz_RAuw5TfEVo
"""

# Loading data
import pandas as pd
train = pd.read_csv("/content/drive/My Drive/Semester7/ML/Project/Layer_7/train.csv")
valid = pd.read_csv("/content/drive/My Drive/Semester7/ML/Project/Layer_7/valid.csv")
test = pd.read_csv("/content/drive/My Drive/Semester7/ML/Project/Layer_7/test.csv")

train.head()

valid.head()

test.head()

"""##Distribution of labels

#Handling Missing Values

##Missing values in training data
"""

# Check for missing values in each label
labels = ["label_1", "label_2", "label_3", "label_4"]
train[labels].isnull().sum()

len(train["label_2"])

from sklearn.impute import SimpleImputer

imputer = SimpleImputer(strategy='mean')
columns = ["label_2"]
imputer.fit(train[columns])

# Insert the missing values with the mean with the nearest integer
train[columns] = imputer.transform(train[columns]).round().astype("int")

# Recheck for missing values in each label to confirm whether there are no missing values left
train[labels].isnull().sum()

"""##Missing values in validation data"""

valid[labels].isnull().sum()

len(valid["label_2"])

imputer = SimpleImputer(strategy='mean')
columns = ["label_2"]
imputer.fit(valid[columns])

# Insert the missing values with the mean with the nearest integer
valid[columns] = imputer.transform(valid[columns]).round().astype("int")
valid[labels].isnull().sum()

"""#Label 1 - Speaker ID"""

# Split X and Y
x_label_1 = train.iloc[:, : -4]
y_label_1 = train["label_1"]
x_valid_label_1 = valid.iloc[:, : -4]
y_valid_label_1 = valid["label_1"]

x_label_1.head()

y_label_1.head()

x_test_label_1 = test.iloc[:, 1:]
x_test_label_1.head()

"""##Class Distribution"""

import matplotlib.pyplot as plt

def plot_class_distribution(y):
  label_counts = y.value_counts()
  plt.figure(figsize=(12, 6))
  label_counts.plot(kind='bar')
  plt.xlabel('Value')
  plt.ylabel('Count')
  plt.title('Class Distribution')
  plt.xticks(rotation=45)
  plt.show()

plot_class_distribution(y_label_1)

"""`label 1` shows an even class distribition.

##Feature Selection
###Check for columns with low variance
"""

from sklearn.feature_selection import VarianceThreshold

def variance_check(x: pd.DataFrame, variance: float) -> []:
  try:
    var_thres = VarianceThreshold(variance)
    var_thres.fit(x)

    const_columns = [column for column in x.columns
                        if column not in x_label_1.columns[var_thres.get_support()]]

    print(f"The number of constant columns is {len(const_columns)}.")
    return const_columns

  except ValueError:
    print(f"No feature exists with variance grater than {variance}.")
    return []

const_columns_label_1 = variance_check(x_label_1, 0.3)

const_columns_label_1 = variance_check(x_label_1, 0.2)

const_columns_label_1 = variance_check(x_label_1, 0.18)

const_columns_label_1 = variance_check(x_label_1, 0.17)

"""Almost all columns have lower variance than 0.17. Therefore, I could not take any feature selection decision based on the variance.

###Check for correlation between features
"""

def correlation_check(x: pd.DataFrame, threshold: float) -> set:
  correlated_features = set()
  correlation_matrix = x.corr()
  for i in range(len(x.columns)):
      for j in range(i):
          if abs(correlation_matrix.iloc[i, j]) > threshold:
              colname = correlation_matrix.columns[i]
              correlated_features.add(colname)
  return correlated_features

corr = correlation_check(x_label_1, 0.9)
print(len(corr))

corr = correlation_check(x_label_1, 0.7)
print(len(corr))

"""There are 4 features with correlation value greater than 0.7. Dropping those columns."""

corr

x_label_1 = x_label_1.drop(corr, axis = 1)

x_valid_label_1 = x_valid_label_1.drop(corr, axis = 1)

x_test_label_1 = x_test_label_1.drop(corr, axis = 1)

"""###Mutual Information Classification"""

from sklearn.feature_selection import mutual_info_classif

def plot_mi(x: pd.DataFrame, y: pd.DataFrame) -> pd.Series:
  mi = pd.Series(mutual_info_classif(x, y))
  mi = mi.sort_values(ascending=False)
  mi.plot.bar(figsize=(32, 12))
  return mi

mi_label_1 = plot_mi(x_label_1, y_label_1)

"""Observing the distribution of the graph, selecting features with mutual information scre greater than 0.06."""

def feature_selection_mi(mi: pd.Series, x: pd.DataFrame, threshold: float) -> pd.DataFrame:
  selected_features = mi[mi > threshold]
  print (f"Selected {selected_features.count()} features out of {len(mi)} features.")
  selected_cols = x.columns[selected_features.index]
  print(selected_cols)
  return selected_cols

selected_col_names_label_1 = feature_selection_mi(mi_label_1, x_label_1, 0.06)

x_label_1_selected = pd.DataFrame(x_label_1, columns = selected_col_names_label_1)
x_valid_label_1_selected = pd.DataFrame(x_valid_label_1, columns = selected_col_names_label_1)

x_test_label_1_selected = pd.DataFrame(x_test_label_1, columns = selected_col_names_label_1)

"""##Modelling

###Model Evaluation
"""

from sklearn.metrics import confusion_matrix, classification_report
import numpy as np

def eval_model(y_actual: pd.DataFrame, y_predicted: pd.DataFrame, print_metrics = False) -> float:
  cm = confusion_matrix(y_actual, y_predicted)
  fp = cm.sum(axis=0) - np.diag(cm)
  fn = cm.sum(axis=1) - np.diag(cm)
  tp = np.diag(cm)
  tn = cm.sum() - (fp + fn + tp)

  accuracy = (tp + tn) / (tp + fp + tn + fn)
  err_rate = (fp + fn) / (tp + fp + tn + fn)
  sensitivity = tp / (tp + fn)
  specificity = tn / (tn + fp)
  precision = tp / (tp + fp)

  recall = sensitivity
  f_score = (2 * precision * recall) / (precision + recall)

  if (print_metrics):
    print(f"Accuracy:      {accuracy.mean():.4f}")
    print(f"Error Rate:    {err_rate.mean():.4f}")
    print(f"Sensitivity:   {sensitivity.mean():.4f}")
    print(f"Specificity:   {specificity.mean():.4f}")
    print(f"Precision:     {precision.mean():.4f}")
    print(f"F1 Score:      {f_score.mean():.4f}")

  return f_score.mean()

from sklearn.model_selection import cross_val_score

def scorer(estimator: any, X: pd.DataFrame, y: pd.DataFrame):
  y_pred = estimator.predict(X)
  f1_score = eval_model(y, y_pred)
  return f1_score

label_1_scores = []

"""###KNN Model"""

from sklearn.neighbors import KNeighborsClassifier

def knn_model(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, k: int):
  knn = KNeighborsClassifier(n_neighbors = k)
  f_score = cross_val_score(knn, x_train, y_train, cv = 5, scoring = scorer)
  knn.fit(x_train, y_train)
  y_pred = knn.predict(x_valid)
  eval_model(y_valid, y_pred, True)
  return [knn, f_score]

knn_label_1, f_score_knn_label_1 = knn_model(x_label_1_selected, y_label_1, x_valid_label_1_selected, y_valid_label_1, 13)
avg_f1_score_knn_label_1 = f_score_knn_label_1.mean()
print(f"Average F1 score of KNN cross validation: {avg_f1_score_knn_label_1}.")
label_1_scores.append([knn_label_1, "KNN", avg_f1_score_knn_label_1])

"""###Random Forest Model"""

from sklearn.ensemble import RandomForestClassifier

def rf_model(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, n_estimators: int, random_state: int):
  rf = RandomForestClassifier(n_estimators = n_estimators, random_state = random_state)
  f_score = cross_val_score(rf, x_train, y_train, cv = 5, scoring = scorer)
  rf.fit(x_train, y_train)
  y_pred = rf.predict(x_valid)
  eval_model(y_valid, y_pred, True)
  return [rf, f_score]

rf_label_1, f_score_rf_label_1 = rf_model(x_label_1_selected, y_label_1, x_valid_label_1_selected, y_valid_label_1, 100, 42)
avg_f1_score_rf_label_1 = f_score_rf_label_1.mean()
print(f"Average F1 score of RF cross validation: {avg_f1_score_rf_label_1}.")
label_1_scores.append([rf_label_1, "Random Forest", avg_f1_score_rf_label_1])

"""###XG Boost Model"""

import xgboost as xgb
import torch

def xgb_model_gpu(x_train, y_train, x_valid, y_valid, n_estimators, random_state):
  xgb_gpu = None
  if (torch.cuda.is_available()):
    xgb_gpu = xgb.XGBClassifier(
        n_estimators = n_estimators,
        random_state = random_state,
        tree_method = 'gpu_hist',
        gpu_id = 0,
    )
  else:
    xgb_gpu = xgb.XGBClassifier(n_estimators = n_estimators, random_state = random_state)

  f_score = cross_val_score(xgb_gpu, x_train, y_train, cv = 5, scoring = scorer)
  xgb_gpu.fit(x_train, y_train)
  y_pred = xgb_gpu.predict(x_valid)
  eval_model(y_valid, y_pred, True)

  return [xgb_gpu, f_score]

from sklearn.preprocessing import LabelEncoder

label_1_encoder = LabelEncoder()
y_label_1_encoded = label_1_encoder.fit_transform(y_label_1.copy(deep = True))
y_valid_label_1_encoded = label_1_encoder.transform(y_valid_label_1.copy(deep = True))

xgb_label_1, f_score_xgb_label_1 = xgb_model_gpu(x_label_1_selected, y_label_1_encoded, x_valid_label_1_selected, y_valid_label_1_encoded, 50, 42)
avg_f1_score_xgb_label_1 = f_score_xgb_label_1.mean()
print(f"Average F1 score of XGB cross validation: {avg_f1_score_xgb_label_1}.")
label_1_scores.append([xgb_label_1, "XGBoost", avg_f1_score_xgb_label_1])

"""###Support Vector Machine Model"""

from sklearn.svm import SVC

def svm_model(x_train: pd.DataFrame, y_train: pd.DataFrame, x_valid: pd.DataFrame, y_valid: pd.DataFrame, kernel: str):
  svm = SVC(kernel = kernel)
  f_score = cross_val_score(svm, x_train, y_train, cv = 5, scoring = scorer)
  svm.fit(x_train, y_train)
  y_pred = svm.predict(x_valid)
  eval_model(y_valid, y_pred, True)
  return [svm, f_score]

kernels = ["linear", "rbf", "poly"]

for i in range(3):
  kernel = kernels[i]
  Kernel = kernel.capitalize()
  print(Kernel)
  svm, f_sc = svm_model(x_label_1_selected, y_label_1, x_valid_label_1_selected, y_valid_label_1, kernel)
  avg_f_sc = f_sc.mean()
  print(f"Average F1 score of SVM {Kernel} cross validation: {avg_f_sc}.")
  label_1_scores.append([svm, f"SVM {Kernel}", avg_f_sc])
  if (i!= 2): print()

"""###Final Model"""

for score in label_1_scores:
  print(score[1:])

def get_best_model(scores):
  max_index = 0
  for i in range(len(scores)):
    if scores[i][2] > scores[max_index][2]:
      max_index = i
  return scores[max_index]

final_model_label_1, final_model_label_1_name, final_model_label_1_f1_score = get_best_model(label_1_scores)
print(f"Selected {final_model_label_1_name} with cross validation score of {final_model_label_1_f1_score}.")
y_test_pred_label_1 = final_model_label_1.predict(x_test_label_1_selected)

def create_csv(y_pred, label_name):
  output_filename = f"/content/drive/My Drive/Semester7/ML/Project/Layer_7/Results/{label_name}.csv"
  combined_data = pd.DataFrame()
  combined_data[label_name] = y_pred
  combined_data.to_csv(output_filename, index=False)

create_csv(y_test_pred_label_1, "label_1")

"""#Label 2"""

x_label_2 = train.iloc[:, : -4]
y_label_2 = train["label_2"]
x_valid_label_2 = valid.iloc[:, : -4]
y_valid_label_2 = valid["label_2"]
x_test_label_2 = test.iloc[:, 1:]

"""## Class Distribition"""

plot_class_distribution(y_label_2)

"""The dataset is imbalanced. Therefore resampling is required. SMOTE will be used."""

from imblearn.over_sampling import SMOTE

smote = SMOTE(sampling_strategy='auto', random_state=42)
x_label_2_resampled, y_label_2_resampled = smote.fit_resample(x_label_2, y_label_2)

plot_class_distribution(y_label_2_resampled)

"""##Feature Selection

###Correlation
"""

x_label_2_resampled = x_label_2_resampled.drop(corr, axis = 1)
x_valid_label_2 = x_valid_label_2.drop(corr, axis = 1)
x_test_label_2 = x_test_label_2.drop(corr, axis = 1)

"""###Mutual Information Classification"""

mi_label_2 = plot_mi(x_label_2_resampled, y_label_2_resampled)

"""The mutual information distribution is similar to `label 1`. Using the threshold as 0.06."""

selected_col_names_label_2 = feature_selection_mi(mi_label_2, x_label_2_resampled, 0.06)

x_label_2_selected = pd.DataFrame(x_label_2_resampled, columns = selected_col_names_label_2)
x_valid_label_2_selected = pd.DataFrame(x_valid_label_2, columns = selected_col_names_label_2)
x_test_label_2_selected = pd.DataFrame(x_test_label_2, columns = selected_col_names_label_2)

"""##Modelling

###KNN Model
"""

label_2_scores = []

knn_label_2, f_score_knn_label_2 = knn_model(x_label_2_selected, y_label_2_resampled, x_valid_label_2_selected, y_valid_label_2, 13)
avg_f1_score_knn_label_2 = f_score_knn_label_2.mean()
print(f"Average F1 score of KNN cross validation: {avg_f1_score_knn_label_2}.")
label_2_scores.append([knn_label_2, "KNN", avg_f1_score_knn_label_2])

"""###Random Forest Model"""

rf_label_2, f_score_rf_label_2 = rf_model(x_label_2_selected, y_label_2_resampled, x_valid_label_2_selected, y_valid_label_2, 100, 42)
avg_f1_score_rf_label_2 = f_score_rf_label_2.mean()
print(f"Average F1 score of RF cross validation: {avg_f1_score_rf_label_2}.")
label_2_scores.append([rf_label_2, "Random Forest", avg_f1_score_rf_label_2])

"""###XG Boost Model"""

label_2_encoder = LabelEncoder()
y_label_2_encoded = label_2_encoder.fit_transform(y_label_2_resampled.copy(deep = True))
y_valid_label_2_encoded = label_2_encoder.transform(y_valid_label_2.copy(deep = True))

xgb_label_2, f_score_xgb_label_2 = xgb_model_gpu(x_label_2_selected, y_label_2_encoded, x_valid_label_2_selected, y_valid_label_2_encoded, 50, 42)
avg_f1_score_xgb_label_2 = f_score_xgb_label_2.mean()
print(f"Average F1 score of XGB cross validation: {avg_f1_score_xgb_label_2}.")
label_2_scores.append([xgb_label_2, "XGBoost", avg_f1_score_xgb_label_2])

"""###SVM Model"""

kernels = ["linear", "rbf", "poly"]

for i in range(3):
  kernel = kernels[i]
  Kernel = kernel.capitalize()
  print(Kernel)
  svm, f_sc = svm_model(x_label_2_selected, y_label_2_resampled, x_valid_label_2_selected, y_valid_label_2, kernel)
  avg_f_sc = f_sc.mean()
  print(f"Average F1 score of SVM {Kernel} cross validation: {avg_f_sc}.")
  label_2_scores.append([svm, f"SVM {Kernel}", avg_f_sc])
  if (i!= 2): print()

"""##Final Model"""

for score in label_2_scores:
  print(score[1:])

final_model_label_2, final_model_label_2_name, final_model_label_2_f1_score = get_best_model(label_2_scores)
print(f"Selected {final_model_label_2_name} with cross validation score of {final_model_label_2_f1_score}.")
y_test_pred_label_2 = final_model_label_2.predict(x_test_label_2_selected)

create_csv(y_test_pred_label_2, "label_2")

"""#Label 3"""

x_label_3 = train.iloc[:, : -4]
y_label_3 = train["label_3"]
x_valid_label_3 = valid.iloc[:, : -4]
y_valid_label_3 = valid["label_3"]
x_test_label_3 = test.iloc[:, 1:]

"""##Class Distribution"""

plot_class_distribution(y_label_3)

"""This dataset is also imbalanced. Using SMOTE."""

smote = SMOTE(sampling_strategy='auto', random_state=42)
x_label_3_resampled, y_label_3_resampled = smote.fit_resample(x_label_3, y_label_3)

plot_class_distribution(y_label_3_resampled)

"""##Feature Selection

###Correlation
"""

x_label_3_resampled = x_label_3_resampled.drop(corr, axis = 1)
x_valid_label_3 = x_valid_label_3.drop(corr, axis = 1)
x_test_label_3 = x_test_label_3.drop(corr, axis = 1)

"""###Mutual Information Classification"""

mi_label_3 = plot_mi(x_label_3_resampled, y_label_3_resampled)

"""Observing the graph, drop the features with mutual information score less than to 0.01."""

selected_col_names_label_3 = feature_selection_mi(mi_label_3, x_label_3_resampled, 0.01)

x_label_3_selected = pd.DataFrame(x_label_3_resampled, columns = selected_col_names_label_3)
x_valid_label_3_selected = pd.DataFrame(x_valid_label_3, columns = selected_col_names_label_3)
x_test_label_3_selected = pd.DataFrame(x_test_label_3, columns = selected_col_names_label_3)

"""##Modeling

###KNN Model
"""

label_3_scores = []

knn_label_3, f_score_knn_label_3 = knn_model(x_label_3_selected, y_label_3_resampled, x_valid_label_3_selected, y_valid_label_3, 13)
avg_f1_score_knn_label_3 = f_score_knn_label_3.mean()
print(f"Average F1 score of KNN cross validation: {avg_f1_score_knn_label_3}.")
label_3_scores.append([knn_label_3, "KNN", avg_f1_score_knn_label_3])

"""###Random Forest Model"""

rf_label_3, f_score_rf_label_3 = rf_model(x_label_3_selected, y_label_3_resampled, x_valid_label_3_selected, y_valid_label_3, 100, 42)
avg_f1_score_rf_label_3 = f_score_rf_label_3.mean()
print(f"Average F1 score of RF cross validation: {avg_f1_score_rf_label_3}.")
label_3_scores.append([rf_label_3, "Random Forest", avg_f1_score_rf_label_3])

"""###XG Boost Model"""

label_3_encoder = LabelEncoder()
y_label_3_encoded = label_3_encoder.fit_transform(y_label_3_resampled.copy(deep = True))
y_valid_label_3_encoded = label_3_encoder.transform(y_valid_label_3.copy(deep = True))

xgb_label_3, f_score_xgb_label_3 = xgb_model_gpu(x_label_3_selected, y_label_3_encoded, x_valid_label_3_selected, y_valid_label_3_encoded, 50, 42)
avg_f1_score_xgb_label_3 = f_score_xgb_label_3.mean()
print(f"Average F1 score of XGB cross validation: {avg_f1_score_xgb_label_3}.")
label_3_scores.append([xgb_label_3, "XGBoost", avg_f1_score_xgb_label_3])

"""###SVM Model"""

kernels = ["linear", "rbf", "poly"]

for i in range(3):
  kernel = kernels[i]
  Kernel = kernel.capitalize()
  print(Kernel)
  svm, f_sc = svm_model(x_label_3_selected, y_label_3_resampled, x_valid_label_3_selected, y_valid_label_3, kernel)
  avg_f_sc = f_sc.mean()
  print(f"Average F1 score of SVM {Kernel} cross validation: {avg_f_sc}.")
  label_3_scores.append([svm, f"SVM {Kernel}", avg_f_sc])
  if (i!= 2): print()

"""###Final Model"""

for score in label_3_scores:
  print(score[1:])

final_model_label_3, final_model_label_3_name, final_model_label_3_f1_score = get_best_model(label_3_scores)
print(f"Selected {final_model_label_3_name} with cross validation score of {final_model_label_3_f1_score}.")
y_test_pred_label_3 = final_model_label_3.predict(x_test_label_3_selected)

create_csv(y_test_pred_label_3, "label_3")

"""#Label 4"""

x_label_4 = train.iloc[:, : -4]
y_label_4 = train["label_4"]
x_valid_label_4 = valid.iloc[:, : -4]
y_valid_label_4 = valid["label_4"]
x_test_label_4 = test.iloc[:, 1:]

"""##Class Distribution"""

plot_class_distribution(y_label_4)

"""Dataset is imbalanced. Using SMOTE."""

smote = SMOTE(sampling_strategy='auto', random_state=42)
x_label_4_resampled, y_label_4_resampled = smote.fit_resample(x_label_4, y_label_4)

plot_class_distribution(y_label_4_resampled)

"""##Feature Selection

###Correlation
"""

x_label_4_resampled = x_label_4_resampled.drop(corr, axis = 1)
x_valid_label_4 = x_valid_label_4.drop(corr, axis = 1)
x_test_label_4 = x_test_label_4.drop(corr, axis = 1)

"""###Mutual Information Classification

Since now there is a large number of data points after SMOTE resampling, mutual information classification took a lot of time. Therefore, it was not used.


###Principal Component Analysis
"""

from sklearn.decomposition import PCA

pca = PCA(n_components = 0.95, svd_solver = 'full')
pca.fit(x_label_4_resampled)
x_label_4_selected = pd.DataFrame(pca.transform(x_label_4_resampled))
x_valid_label_4_selected = pd.DataFrame(pca.transform(x_valid_label_4))
x_test_label_4_selected = pd.DataFrame(pca.transform(x_test_label_4))
x_label_4_selected.shape

"""##Modelling

###KNN Model
"""

label_4_scores = []

knn_label_4, f_score_knn_label_4 = knn_model(x_label_4_selected, y_label_4_resampled, x_valid_label_4_selected, y_valid_label_4, 13)
avg_f1_score_knn_label_4 = f_score_knn_label_4.mean()
print(f"Average F1 score of KNN cross validation: {avg_f1_score_knn_label_4}.")
label_4_scores.append([knn_label_4, "KNN", avg_f1_score_knn_label_4])

"""###Random Forest Model"""

rf_label_4, f_score_rf_label_4 = rf_model(x_label_4_selected, y_label_4_resampled, x_valid_label_4_selected, y_valid_label_4, 100, 42)
avg_f1_score_rf_label_4 = f_score_rf_label_4.mean()
print(f"Average F1 score of RF cross validation: {avg_f1_score_rf_label_4}.")
label_4_scores.append([rf_label_4, "Random Forest", avg_f1_score_rf_label_4])

"""###Final Model"""

for score in label_4_scores:
  print(score[1:])

final_model_label_4, final_model_label_4_name, final_model_label_4_f1_score = get_best_model(label_4_scores)
print(f"Selected {final_model_label_4_name} with cross validation score of {final_model_label_4_f1_score}.")
y_test_pred_label_4 = final_model_label_4.predict(x_test_label_4_selected)

create_csv(y_test_pred_label_4, "label_4")

"""#Hyper Parameter Tuning

Since all the selected models have accuracy for validation dataset greater than 0.8 and F1 scores for validation dataset as well as cross validation is acceptable, hyper parameter tuning was not considered.

#Final Results
"""

output_filename = f"/content/drive/My Drive/Semester7/ML/Project/Layer_7/Results/final.csv"
final_combined_data = pd.DataFrame()
final_combined_data["ID"] = test["ID"]
for i in range(1, 5):
  label_name = f"label_{i}"
  final_combined_data[label_name] = pd.read_csv(f"/content/drive/My Drive/Semester7/ML/Project/Layer_7/Results/{label_name}.csv")[label_name]
final_combined_data.to_csv(output_filename, index=False)
